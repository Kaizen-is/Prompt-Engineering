{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt Engineering"
      ],
      "metadata": {
        "id": "A6HHMTXw8_z3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### AI Concepts"
      ],
      "metadata": {
        "id": "5t2mIiZ89POD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Larg Language Models"
      ],
      "metadata": {
        "id": "B0P5fGvK9aiZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLMs are advanced artificial intelligence systems that deep learning, speciffically neural networks like transformers, to understand, process, and generate human language. They are trained on enormous datasets consisting of books, websites, articles, and other text to learn grammar, facts, reasoning patterns, and even subtle context. Examples include GPT-4, BERT, and PaLM."
      ],
      "metadata": {
        "id": "pR_-Lo8i9e2d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generative AI"
      ],
      "metadata": {
        "id": "0ICZXv7H-YNZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This refers to AI models designed to generate new, original content. Generative AI can create text, images, music, video, or code. It uses patterns learned during training to produce content that mimics human creativity. Techniques often include deep learning models such as Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and transformer-based models like GPT."
      ],
      "metadata": {
        "id": "erxhWeWJ-bYO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Token"
      ],
      "metadata": {
        "id": "EK8ejRKc-dXC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In natural language processing (NLP), a token is the smallest unit the model reads. Tokens can be whole words (\"cat\"), parts of words (\"ing\" in \"running\"), or even punctuation marks. For example, \"ChatGPT is smart.\" might be split into 5 tokens: [\"Chat\", \"G\", \"PT\", \" is\", \" smart\", \".\"]. Tokenization helps the model understand and manage input efficiently."
      ],
      "metadata": {
        "id": "iVrHtygx-gNc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reinforcement Learning (RL)"
      ],
      "metadata": {
        "id": "ohX3yWOa-jkc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RL is a machine learning paradigm where an agent interacts with an environment, makes decisions (actions), and receives feedback in the form of rewards or penalties. Over time, it learns to maximize rewards. It’s often used in robotics, games (e.g., AlphaGo), and is also used in fine-tuning language models via Reinforcement Learning from Human Feedback (RLHF), where human preferences help shape the model’s responses."
      ],
      "metadata": {
        "id": "4FR2UTvj-m8A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-tuning"
      ],
      "metadata": {
        "id": "NAT9Igh1-rKu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the process of taking a pre-trained model (like GPT-4) and training it further on a smaller, task-specific dataset. It allows the model to adapt to specialized tasks (like legal document summarization, medical Q&A, or customer service). Fine-tuning requires fewer data and resources compared to training a model from scratch and leads to improved performance on targeted tasks."
      ],
      "metadata": {
        "id": "SXe3Z9hT-uJc"
      }
    }
  ]
}